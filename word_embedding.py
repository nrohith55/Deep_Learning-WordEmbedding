# -*- coding: utf-8 -*-
"""Word_Embedding.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Rq5Uz80rAVuHNhYCEp-ueDChpMhiAwtU

Word Embedding Techniques using Embedding layer in Keras
"""

from tensorflow.keras.preprocessing.text import one_hot

sen=['the glass of milk',
     'the glass of juice',
     'the cup of tea',
     'I am a good boy',
     'I am a good developer',
     'understand the meaning of words',
     'your videos are good']

print(sen)

voc_size=10000 #vocabulary size

"""One Hot Represemtation"""

one_hot_repr=[one_hot(words,voc_size) for words in sen]
print(one_hot_repr)

"""Word Embedding Representation"""

from tensorflow.keras.models import  Sequential
from tensorflow.keras.layers import Embedding
from tensorflow.keras.preprocessing.sequence import pad_sequences

import numpy as np

sen_lenghth=8
embedded_docs=pad_sequences (one_hot_repr,maxlen=sen_lenghth,padding='pre')
print(embedded_docs)

model=Sequential()
model.add(Embedding(voc_size,10,input_length=sen_lenghth))

model.compile()

model.summary()

print(model.predict(embedded_docs))

print(embedded_docs[0])

print(model.predict(embedded_docs)[0])

